{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 基于经典网络架构训练图像分类模型"
   ],
   "id": "5389b5ac32c7cacb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 数据预处理部分：\n",
    "\n",
    "- 数据增强：torchvision中transforms模块自带功能，比较实用\n",
    "- 数据预处理：torchvision中transforms也帮我们实现好了，直接调用即可\n",
    "- DataLoader模块直接读取batch数据\n",
    "\n",
    "### 网络模块设置：\n",
    "\n",
    "- 加载预训练模型，torchvision中有很多经典网络架构，调用起来十分方便，并且可以用人家训练好的权重参数来继续训练，也就是所谓的迁移学习\n",
    "- 需要注意的是别人训练好的任务跟咱们的可不是完全一样，需要把最后的head层改一改，一般也就是最后的全连接层，改成咱们自己的任务\n",
    "- 训练时可以全部重头训练，也可以只训练最后咱们任务的层，因为前几层都是做特征提取的，本质任务目标是一致的\n",
    "\n",
    "### 网络模型保存与测试\n",
    "- 模型保存的时候可以带有选择性，例如在验证集中如果当前效果好则保存\n",
    "- 读取模型进行实际测试\n",
    "\n",
    "![title](1.png)"
   ],
   "id": "35484a9163310e02"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-08T01:12:33.848391Z",
     "end_time": "2025-04-08T01:12:33.941407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms, models, datasets\n",
    "import imageio\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "from PIL import Image"
   ],
   "id": "83db4b5dc82f3e7e",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-08T01:12:33.850550Z",
     "end_time": "2025-04-08T01:12:33.941513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_dir = './flower_data/'"
   ],
   "id": "f70237866ed84079",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 制作好数据源：\n",
    "- data_transforms中指定了所有图像预处理操作\n",
    "- ImageFolder假设所有的文件按文件夹保存好，每个文件夹下面存贮同一类别的图片，文件夹的名字为分类的名字"
   ],
   "id": "76e1e1df52e284f9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-08T01:12:33.857169Z",
     "end_time": "2025-04-08T01:12:33.953018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_transforms = {\n",
    "    'train':\n",
    "        transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize([96, 96]),  #对所有图片重新调整大小，保持一致\n",
    "                transforms.RandomRotation(45),  #随机旋转,-45°到45°之间随机选 ，强化数据，生成更多数据\n",
    "                transforms.CenterCrop(64),  #从中心选择，裁剪图片保留64\n",
    "                transforms.RandomHorizontalFlip(p=0.5),  #随机水平翻转\n",
    "                transforms.RandomVerticalFlip(p=0.5),  #随机垂直翻转\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.1),  #brightness 亮度，contrast 对比度，saturation 饱和度 hue 色相\n",
    "                transforms.RandomGrayscale(p=0.025),  # 随机将一些转换为灰度图，p随即率\n",
    "                transforms.ToTensor(),  #转换为tensor数据，ToTensor() 会将图像像素值从 [0, 255] 缩放到 [0, 1]\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  #均值，标准差 三个通道的均值，标准差，数值是自然图像得到的统计结果\n",
    "            ]\n",
    "        ),\n",
    "    'valid':\n",
    "        transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize([64, 64]),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.4488, 0.4371, 0.4040], [0.229, 0.224, 0.225])\n",
    "            ]\n",
    "        )\n",
    "}"
   ],
   "id": "a35bc96dea80b57b",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-08T01:12:33.860162Z",
     "end_time": "2025-04-08T01:12:34.016378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bat_size = 128\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'valid']}\n",
    "dataloaders = {x: DataLoader(image_datasets[x], batch_size=bat_size, shuffle=True) for x in ['train', 'valid']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'valid']}\n",
    "class_names = image_datasets['train'].classes"
   ],
   "id": "4595bfb9fbccf25f",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-08T01:12:33.921506Z",
     "end_time": "2025-04-08T01:12:34.016622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "image_datasets"
   ],
   "id": "c791b9c71224aaea",
   "outputs": [
    {
     "data": {
      "text/plain": "{'train': Dataset ImageFolder\n     Number of datapoints: 6552\n     Root location: ./flower_data/train\n     StandardTransform\n Transform: Compose(\n                Resize(size=[96, 96], interpolation=bilinear, max_size=None, antialias=True)\n                RandomRotation(degrees=[-45.0, 45.0], interpolation=nearest, expand=False, fill=0)\n                CenterCrop(size=(64, 64))\n                RandomHorizontalFlip(p=0.5)\n                RandomVerticalFlip(p=0.5)\n                ColorJitter(brightness=(0.8, 1.2), contrast=(0.9, 1.1), saturation=None, hue=None)\n                RandomGrayscale(p=0.025)\n                ToTensor()\n                Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n            ),\n 'valid': Dataset ImageFolder\n     Number of datapoints: 818\n     Root location: ./flower_data/valid\n     StandardTransform\n Transform: Compose(\n                Resize(size=[64, 64], interpolation=bilinear, max_size=None, antialias=True)\n                ToTensor()\n                Normalize(mean=[0.4488, 0.4371, 0.404], std=[0.229, 0.224, 0.225])\n            )}"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-08T01:12:33.924044Z",
     "end_time": "2025-04-08T01:12:34.016683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataloaders"
   ],
   "id": "d2c93774ff1b3aed",
   "outputs": [
    {
     "data": {
      "text/plain": "{'train': <torch.utils.data.dataloader.DataLoader at 0x2953c0da0>,\n 'valid': <torch.utils.data.dataloader.DataLoader at 0x2953c15e0>}"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-08T01:12:33.927309Z",
     "end_time": "2025-04-08T01:12:34.017778Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'train': 6552, 'valid': 818}"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 62,
   "source": [
    "dataset_sizes"
   ],
   "id": "8c12a718b968ead0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "读取标签对应的实际名字"
   ],
   "id": "5f1df2b600ede1c9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-08T01:12:33.929563Z",
     "end_time": "2025-04-08T01:12:34.017875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('cat_to_name.json', 'r') as f:\n",
    "    cat_to_name = json.load(f)"
   ],
   "id": "b31d110171fbebfd",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-08T01:12:33.932667Z",
     "end_time": "2025-04-08T01:12:34.017980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cat_to_name"
   ],
   "id": "96694ea1989be75",
   "outputs": [
    {
     "data": {
      "text/plain": "{'21': 'fire lily',\n '3': 'canterbury bells',\n '45': 'bolero deep blue',\n '1': 'pink primrose',\n '34': 'mexican aster',\n '27': 'prince of wales feathers',\n '7': 'moon orchid',\n '16': 'globe-flower',\n '25': 'grape hyacinth',\n '26': 'corn poppy',\n '79': 'toad lily',\n '39': 'siam tulip',\n '24': 'red ginger',\n '67': 'spring crocus',\n '35': 'alpine sea holly',\n '32': 'garden phlox',\n '10': 'globe thistle',\n '6': 'tiger lily',\n '93': 'ball moss',\n '33': 'love in the mist',\n '9': 'monkshood',\n '102': 'blackberry lily',\n '14': 'spear thistle',\n '19': 'balloon flower',\n '100': 'blanket flower',\n '13': 'king protea',\n '49': 'oxeye daisy',\n '15': 'yellow iris',\n '61': 'cautleya spicata',\n '31': 'carnation',\n '64': 'silverbush',\n '68': 'bearded iris',\n '63': 'black-eyed susan',\n '69': 'windflower',\n '62': 'japanese anemone',\n '20': 'giant white arum lily',\n '38': 'great masterwort',\n '4': 'sweet pea',\n '86': 'tree mallow',\n '101': 'trumpet creeper',\n '42': 'daffodil',\n '22': 'pincushion flower',\n '2': 'hard-leaved pocket orchid',\n '54': 'sunflower',\n '66': 'osteospermum',\n '70': 'tree poppy',\n '85': 'desert-rose',\n '99': 'bromelia',\n '87': 'magnolia',\n '5': 'english marigold',\n '92': 'bee balm',\n '28': 'stemless gentian',\n '97': 'mallow',\n '57': 'gaura',\n '40': 'lenten rose',\n '47': 'marigold',\n '59': 'orange dahlia',\n '48': 'buttercup',\n '55': 'pelargonium',\n '36': 'ruby-lipped cattleya',\n '91': 'hippeastrum',\n '29': 'artichoke',\n '71': 'gazania',\n '90': 'canna lily',\n '18': 'peruvian lily',\n '98': 'mexican petunia',\n '8': 'bird of paradise',\n '30': 'sweet william',\n '17': 'purple coneflower',\n '52': 'wild pansy',\n '84': 'columbine',\n '12': \"colt's foot\",\n '11': 'snapdragon',\n '96': 'camellia',\n '23': 'fritillary',\n '50': 'common dandelion',\n '44': 'poinsettia',\n '53': 'primula',\n '72': 'azalea',\n '65': 'californian poppy',\n '80': 'anthurium',\n '76': 'morning glory',\n '37': 'cape flower',\n '56': 'bishop of llandaff',\n '60': 'pink-yellow dahlia',\n '82': 'clematis',\n '58': 'geranium',\n '75': 'thorn apple',\n '41': 'barbeton daisy',\n '95': 'bougainvillea',\n '43': 'sword lily',\n '83': 'hibiscus',\n '78': 'lotus lotus',\n '88': 'cyclamen',\n '94': 'foxglove',\n '81': 'frangipani',\n '74': 'rose',\n '89': 'watercress',\n '73': 'water lily',\n '46': 'wallflower',\n '77': 'passion flower',\n '51': 'petunia'}"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 加载models中提供的模型，并且直接用训练的好权重当做初始化参数\n",
    "- 第一次执行需要下载，可能会比较慢，我会提供给大家一份下载好的，可以直接放到相应路径"
   ],
   "id": "1e96729363a53f68"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-08T01:12:33.935204Z",
     "end_time": "2025-04-08T01:12:34.018009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = 'resnet'  # 可选的比较多[resnet,densenet,vgg,squeezenet...] 经典网络，目前使用较广泛的一个是reshet\n",
    "feature_extract = True  # 都用人家特征，先不更新"
   ],
   "id": "6c61925119b4e26",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-08T01:12:33.937909Z",
     "end_time": "2025-04-08T01:12:34.027376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print('cuda is not available training on cpu')\n",
    "else:\n",
    "    print('cuda is available training on gpu')\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ],
   "id": "504f86383f0617f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is not available training on cpu\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#模型参数不更新，除了输出层先冻结"
   ],
   "id": "23baf71a5b4ad9df"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-08T01:12:33.941394Z",
     "end_time": "2025-04-08T01:12:34.027490Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ],
   "id": "a731e9db4a56e7c",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-08T01:12:33.943069Z",
     "end_time": "2025-04-08T01:12:34.101161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_fit = models.resnet18()  #18层，速度快，效果差，性能好可以选大的\n",
    "model_fit"
   ],
   "id": "9e69069d82b53116",
   "outputs": [
    {
     "data": {
      "text/plain": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "输出结果可以看到，resnet18的最后一层fc全连接的输出是 1000，而我们需要的是102分类，所以需要修改"
   ],
   "id": "b80f85839ce485d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-08T01:12:34.099170Z",
     "end_time": "2025-04-08T01:12:34.101268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def initialize_model(model_name, num_class, feature_extract, use_pretrained=True):\n",
    "    model_fit = models.resnet18(pretrained=use_pretrained)\n",
    "    set_parameter_requires_grad(model_fit,\n",
    "                                feature_extract)  # feature_extract  决定是否 冻结模型参数。True 参数不更新，仅训练新添加的分类层 False 微调模式\n",
    "    num_ftrs = model_fit.fc.in_features  # fc的输入\n",
    "    # 后增加的，requires_grad依旧为true\n",
    "    model_fit.fc = nn.Linear(num_ftrs, num_class)  #输出类别数量 102\n",
    "    input_size = 64  #输入\n",
    "    return model_fit, input_size"
   ],
   "id": "11652e24b5115d36",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [],
   "id": "6642ce175d6173f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-08T01:12:34.102697Z",
     "end_time": "2025-04-08T01:12:34.304645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_ft, input_size = initialize_model(model_name, 102, feature_extract)\n",
    "model_fit = model_ft.to(device)\n",
    "\n",
    "#模型保存\n",
    "filename = 'checkpoint.pth'\n",
    "params_to_update = model_ft.parameters()\n",
    "#是否训练所有层\n",
    "if (feature_extract):\n",
    "    params_to_update = []\n",
    "    for name, param in model_fit.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            params_to_update.append(param)\n",
    "            print('\\t', 1, name)\n",
    "else:\n",
    "    for name, param in model_fit.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print('\\t', 2, name)"
   ],
   "id": "ea1827e6451bcaa1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 1 fc.weight\n",
      "\t 1 fc.bias\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-08T01:12:34.308167Z",
     "end_time": "2025-04-08T01:12:34.310159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#优化器配置\n",
    "optimizer_fit = optim.Adam(params_to_update, lr=1e-2)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer_fit, step_size=10,\n",
    "                                      gamma=0.1)  #  step_size=10,   # 每隔多少 epoch 调整一次学习率    gamma=0.1       # 学习率衰减的乘法因子\n",
    "criterion = nn.CrossEntropyLoss()  #损失函数，适用于分类任务"
   ],
   "id": "6353042df84c6506",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 训练模块"
   ],
   "id": "6a3f16050c98aa9e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-08T01:12:34.315571Z",
     "end_time": "2025-04-08T01:12:34.317537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, filename='best.pt'):\n",
    "    #记录时间   \n",
    "    since = time.time()\n",
    "    #记录验证集预测结果最好的 初始化\n",
    "    best_acc = 0\n",
    "    #模型放cpu/gpu\n",
    "    model_fit.to(device)\n",
    "    #训练过程需要打印指标 验证集acc，训练集acc，各自损失\n",
    "    train_losses, train_acc = [], []\n",
    "    validation_losses, validation_acc = [], []\n",
    "    #学习率获取\n",
    "    l_r = [optimizer_fit.param_groups[0]['lr']]\n",
    "    #初始化最好的模型 copy state_dict\n",
    "    best_wt = copy.deepcopy(model_fit.state_dict())\n",
    "    #遍历epochs\n",
    "    print(num_epochs)\n",
    "    for epoch in range(num_epochs):\n",
    "        #打印当前epoch/epoch总数量\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('*' * 10)\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            #分别求训练集，验证集 acc 和loss    \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs.to(device)\n",
    "                labels.to(device)\n",
    "                #清零\n",
    "                optimizer.zero_grad()\n",
    "                #只要训练的时候计算和更新进度\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                preds = torch.max(outputs, 1)[1]  #第一个1是在一行中找最大的列，第二个是下标\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)  #size(0) 张量第0维的大小，当前batch的数量\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)  # 算平均\n",
    "            epoch_acc = running_corrects / len(dataloaders[phase].dataset)\n",
    "            time_elapsed = time.time() - since  #一个epoch我浪费了多少时间\n",
    "            print('Time elapsed {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            #得到最好的模型\n",
    "            if (phase == 'valid' and epoch_acc > best_acc):\n",
    "                best_acc = epoch_acc\n",
    "                best_wt = copy.deepcopy(model_fit.state_dict())\n",
    "                state = {\n",
    "                    'state_dict': model_fit.state_dict(),\n",
    "                    'best_acc': best_acc,\n",
    "                    'optimizer': optimizer_fit.state_dict()\n",
    "                }\n",
    "                torch.save(state, filename)\n",
    "            if phase == 'valid':\n",
    "                validation_acc.append(epoch_acc)\n",
    "                validation_losses.append(epoch_loss)\n",
    "            if phase == 'test':\n",
    "                train_acc.append(epoch_acc)\n",
    "                train_losses.append(epoch_loss)\n",
    "\n",
    "        print('Optimizer learning rate : {:.7f}'.format(optimizer.param_groups[0]['lr']))\n",
    "        l_r.append(optimizer_fit.param_groups[0]['lr'])\n",
    "        scheduler.step()  #进行学习率衰减\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    #训练完后用最好的一次做模型最终的结果，一会测试\n",
    "    model.load_state_dict(best_wt)\n",
    "    return model_fit, validation_losses, validation_acc, train_losses, train_acc, l_r"
   ],
   "id": "c13ce17d215f50f3",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-04-07T10:35:46.356678Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "model_fit, validation_losses, validation_acc, train_losses, train_acc, l_r = train_model(model_fit, dataloaders,\n",
    "                                                                                         criterion, optimizer_fit,\n",
    "                                                                                         num_epochs=20)"
   ],
   "id": "f6952cad5cc4af0d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "Epoch 1/20\n",
      "**********\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "#得到最后一层权重后，训练所有层\n",
    "for param in model_fit.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer_fit = optimizer_fit.Adam(model_fit.parameters(), lr=1e-2)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer_fit, step_size=7, gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "id": "9ea07fd44acb4943",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "#加载之前的权重参数\n",
    "checkpoint = torch.load(filename)\n",
    "best_acc = checkpoint['best_acc']\n",
    "model_fit.load_state_dict(checkpoint['state_dict'])"
   ],
   "id": "340278940cebf6aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "model_fit, validation_losses, validation_acc, train_losses, train_acc, l_r = train_model(model_fit, dataloaders,\n",
    "                                                                                         criterion, optimizer_fit,\n",
    "                                                                                         num_epochs=20)"
   ],
   "id": "ff83f3a6c523d876",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "model_ft, input_size = initialize_model(model_name, 102, feature_extract, use_pretrained=True)\n",
    "\n",
    "# GPU模式\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "#保存文件的名字\n",
    "filename = 'best.pt'\n",
    "\n",
    "# 加载模型\n",
    "checkpoint = torch.load(filename)\n",
    "best_acc = checkpoint['best_acc']\n",
    "model_ft.load_state_dict(checkpoint['state_dict'])"
   ],
   "id": "1560e72e6c65e9c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 测试数据预处理\n",
    "\n",
    "- 测试数据处理方法需要跟训练时一致才可以 \n",
    "- crop操作的目的是保证输入的大小是一致的\n",
    "- 标准化操作也是必须的，用跟训练数据相同的mean和std,但是需要注意一点训练数据是在0-1上进行标准化，所以测试数据也需要先归一化\n",
    "- 最后一点，PyTorch中颜色通道是第一个维度，跟很多工具包都不一样，需要转换\n"
   ],
   "id": "4791d671bf01e728"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "dataiter = iter(dataloaders['valid'])\n",
    "images, laels = dataiter.next()\n",
    "model_ft.eval()\n",
    "if train_on_gpu:\n",
    "    output = model_ft(images.cuda)\n",
    "else:\n",
    "    output = model_ft(images)"
   ],
   "id": "48ce437999c85200",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "output.shape"
   ],
   "id": "e17780e5c845b2d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "_, pred = torch.max(output, 1)"
   ],
   "id": "b81e46b329fe543d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "preds = np.squeeze(pred.numpy()) if not train_on_gpu else np.squeeze(pred.cpu().numpy())\n",
    "preds"
   ],
   "id": "d28bfcfa7b15a666",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#展示预测结果"
   ],
   "id": "65a30ddf717df0f4"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def im_convert(tensor):\n",
    "    image = tensor.to('cpu').clone().detach()  #画图需要使用numpy 使用的cpu\n",
    "    image = image.numpy().squeeze() # 转换为numpy squeeze 压缩1的维度 1*3*64*64-> 3*64*64\n",
    "    image = image.transpose(1, 2, 0) # plt 中对应的长宽通道，原来的是通道长宽\n",
    "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406)) #对标准化后的数据还原\n",
    "    image = image.clip(0, 1)# 防止还原后的数据超过9，1范围"
   ],
   "id": "3ee12d17fb99f10f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "fig = plt.figure(figsize=(20, 20))\n",
    "columns = 4\n",
    "rows = 2\n",
    "\n",
    "for idx in range(columns * rows):\n",
    "    ax = fig.add_subplot(rows, columns, idx + 1, xticks=[], yticks=[])\n",
    "    plt.imshow(im_convert(images[idx]))\n",
    "    ax.set_title(\"{} ({})\".format(cat_to_name[str(preds[idx])], cat_to_name[str(cat_to_name[idx].item())]),\n",
    "                 color=(\"green\" if cat_to_name[str(preds[idx])] == cat_to_name[str(cat_to_name[idx].item())] else \"red\"))\n",
    "plt.show()"
   ],
   "id": "b9f6d2548018e4f3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
